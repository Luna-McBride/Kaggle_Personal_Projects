{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Spark Project: Tweet Sentiment Analysis"},{"metadata":{},"cell_type":"markdown","source":"# NOTE: This notebook was meant to work with Spark. I also had a ML aspect with NLP, but Kaggle could not handle it. It seems the best course of action in this case would be to use a different platform, perhaps also with SystemML."},{"metadata":{},"cell_type":"markdown","source":"Coded by Luna McBride\n\nThe point of this project is to work with Apache Spark's features."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport spacy # NLP\nimport re # regular expressions\nimport html # HTML content, like &amp;\nfrom spacy.lang.en.stop_words import STOP_WORDS # stopwords\n\nnlp = spacy.load('en_core_web_lg') #Load spacy, up here so I do not have to load it constantly\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Spark Handling"},{"metadata":{},"cell_type":"markdown","source":"Source for spark handling in Kaggle: https://www.kaggle.com/tylerx/machine-learning-with-spark"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install -U sparkmagic #Install or update sparkmagic, for spark jupyter displays\n!pip install -U pyspark #Install or update pyspark","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting sparkmagic\n  Downloading sparkmagic-0.17.0.tar.gz (37 kB)\nCollecting hdijupyterutils>=0.6\n  Downloading hdijupyterutils-0.17.0.tar.gz (4.6 kB)\nCollecting autovizwidget>=0.6\n  Downloading autovizwidget-0.17.0.tar.gz (8.3 kB)\nRequirement already satisfied, skipping upgrade: ipython>=4.0.2 in /opt/conda/lib/python3.7/site-packages (from sparkmagic) (7.13.0)\nRequirement already satisfied, skipping upgrade: nose in /opt/conda/lib/python3.7/site-packages (from sparkmagic) (1.3.7)\nRequirement already satisfied, skipping upgrade: mock in /opt/conda/lib/python3.7/site-packages (from sparkmagic) (3.0.5)\nRequirement already satisfied, skipping upgrade: pandas>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from sparkmagic) (1.1.3)\nRequirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.7/site-packages (from sparkmagic) (1.18.5)\nRequirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from sparkmagic) (2.23.0)\nRequirement already satisfied, skipping upgrade: ipykernel in /opt/conda/lib/python3.7/site-packages (from sparkmagic) (5.1.1)\nRequirement already satisfied, skipping upgrade: ipywidgets>5.0.0 in /opt/conda/lib/python3.7/site-packages (from sparkmagic) (7.5.1)\nRequirement already satisfied, skipping upgrade: notebook>=4.2 in /opt/conda/lib/python3.7/site-packages (from sparkmagic) (5.5.0)\nRequirement already satisfied, skipping upgrade: tornado>=4 in /opt/conda/lib/python3.7/site-packages (from sparkmagic) (5.0.2)\nCollecting requests_kerberos>=0.8.0\n  Downloading requests_kerberos-0.12.0-py2.py3-none-any.whl (14 kB)\nRequirement already satisfied, skipping upgrade: jupyter>=1 in /opt/conda/lib/python3.7/site-packages (from hdijupyterutils>=0.6->sparkmagic) (1.0.0)\nRequirement already satisfied, skipping upgrade: plotly>=3 in /opt/conda/lib/python3.7/site-packages (from autovizwidget>=0.6->sparkmagic) (4.12.0)\nRequirement already satisfied, skipping upgrade: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.2->sparkmagic) (0.7.5)\nRequirement already satisfied, skipping upgrade: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.2->sparkmagic) (0.1.0)\nRequirement already satisfied, skipping upgrade: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.2->sparkmagic) (4.3.3)\nRequirement already satisfied, skipping upgrade: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.2->sparkmagic) (0.15.2)\nRequirement already satisfied, skipping upgrade: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.2->sparkmagic) (3.0.5)\nRequirement already satisfied, skipping upgrade: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.2->sparkmagic) (4.4.2)\nRequirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.2->sparkmagic) (4.8.0)\nRequirement already satisfied, skipping upgrade: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.2->sparkmagic) (46.1.3.post20200325)\nRequirement already satisfied, skipping upgrade: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.2->sparkmagic) (2.6.1)\nRequirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from mock->sparkmagic) (1.14.0)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.17.1->sparkmagic) (2.8.1)\nRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.17.1->sparkmagic) (2019.3)\nRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->sparkmagic) (3.0.4)\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->sparkmagic) (1.25.9)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->sparkmagic) (2020.6.20)\nRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->sparkmagic) (2.9)\nRequirement already satisfied, skipping upgrade: jupyter-client in /opt/conda/lib/python3.7/site-packages (from ipykernel->sparkmagic) (6.1.3)\nRequirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>5.0.0->sparkmagic) (3.5.1)\nRequirement already satisfied, skipping upgrade: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>5.0.0->sparkmagic) (5.0.6)\nRequirement already satisfied, skipping upgrade: terminado>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.2->sparkmagic) (0.8.3)\nRequirement already satisfied, skipping upgrade: jinja2 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.2->sparkmagic) (2.11.2)\nRequirement already satisfied, skipping upgrade: Send2Trash in /opt/conda/lib/python3.7/site-packages (from notebook>=4.2->sparkmagic) (1.5.0)\nRequirement already satisfied, skipping upgrade: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.2->sparkmagic) (19.0.0)\nRequirement already satisfied, skipping upgrade: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from notebook>=4.2->sparkmagic) (0.2.0)\nRequirement already satisfied, skipping upgrade: jupyter-core>=4.4.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.2->sparkmagic) (4.6.3)\nRequirement already satisfied, skipping upgrade: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.2->sparkmagic) (5.6.1)\nRequirement already satisfied, skipping upgrade: cryptography>=1.3; python_version != \"3.3\" in /opt/conda/lib/python3.7/site-packages (from requests_kerberos>=0.8.0->sparkmagic) (2.8)\nCollecting pykerberos<2.0.0,>=1.1.8; sys_platform != \"win32\"\n  Downloading pykerberos-1.2.1.tar.gz (24 kB)\nRequirement already satisfied, skipping upgrade: jupyter-console in /opt/conda/lib/python3.7/site-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (6.1.0)\nRequirement already satisfied, skipping upgrade: qtconsole in /opt/conda/lib/python3.7/site-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (4.7.3)\nRequirement already satisfied, skipping upgrade: retrying>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from plotly>=3->autovizwidget>=0.6->sparkmagic) (1.3.3)\nRequirement already satisfied, skipping upgrade: parso>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython>=4.0.2->sparkmagic) (0.5.2)\nRequirement already satisfied, skipping upgrade: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.2->sparkmagic) (0.1.9)\nRequirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.2->sparkmagic) (0.6.0)\nRequirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic) (3.2.0)\nRequirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->notebook>=4.2->sparkmagic) (1.1.1)\nRequirement already satisfied, skipping upgrade: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.2->sparkmagic) (3.1.4)\nRequirement already satisfied, skipping upgrade: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.2->sparkmagic) (0.6.0)\nRequirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.2->sparkmagic) (0.3)\nRequirement already satisfied, skipping upgrade: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.2->sparkmagic) (0.4.4)\nRequirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.2->sparkmagic) (1.4.2)\nRequirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.2->sparkmagic) (0.8.4)\nRequirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=1.3; python_version != \"3.3\"->requests_kerberos>=0.8.0->sparkmagic) (1.14.0)\nRequirement already satisfied, skipping upgrade: qtpy in /opt/conda/lib/python3.7/site-packages (from qtconsole->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (1.9.0)\n","name":"stdout"},{"output_type":"stream","text":"Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic) (2.0.0)\nRequirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic) (0.16.0)\nRequirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic) (19.3.0)\nRequirement already satisfied, skipping upgrade: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.2->sparkmagic) (0.5.1)\nRequirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=1.3; python_version != \"3.3\"->requests_kerberos>=0.8.0->sparkmagic) (2.20)\nRequirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic) (3.1.0)\nBuilding wheels for collected packages: sparkmagic, hdijupyterutils, autovizwidget, pykerberos\n  Building wheel for sparkmagic (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sparkmagic: filename=sparkmagic-0.17.0-py3-none-any.whl size=60489 sha256=c9f27a6bda71ae002cfcba2b71c275fd7b2b9412eba4b3482ca94734da1db9c6\n  Stored in directory: /root/.cache/pip/wheels/d8/b0/70/a7689cd13911f373341bc8c60f6fc195a6cc308b4118e00eb0\n  Building wheel for hdijupyterutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for hdijupyterutils: filename=hdijupyterutils-0.17.0-py3-none-any.whl size=7696 sha256=527e32af7ac9a4de00c015e69ce4cbd78337c35d2296167f2ae6ea4e28e668c4\n  Stored in directory: /root/.cache/pip/wheels/b6/30/53/dc0c8ec25a55ca1f0fc7a6251d36acfbb212d7c38562bd1ce5\n  Building wheel for autovizwidget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for autovizwidget: filename=autovizwidget-0.17.0-py3-none-any.whl size=14547 sha256=0daf34197a3112291866ce889decac87ab028d80cf9688838aaf0b33c81721e1\n  Stored in directory: /root/.cache/pip/wheels/ec/a0/2d/21d7ff3bfe851a920b49b677e5bed4d4885dd620d8e3427397\n  Building wheel for pykerberos (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pykerberos: filename=pykerberos-1.2.1-cp37-cp37m-linux_x86_64.whl size=71816 sha256=d03196b9712ad050bec56213eab1d04f97dc727d01b71cd5f1c3061f9ffbff88\n  Stored in directory: /root/.cache/pip/wheels/49/70/e2/33a356bfc619e7cfa71c2a14f6fb976c0963d1bfa02e885151\nSuccessfully built sparkmagic hdijupyterutils autovizwidget pykerberos\nInstalling collected packages: hdijupyterutils, autovizwidget, pykerberos, requests-kerberos, sparkmagic\nSuccessfully installed autovizwidget-0.17.0 hdijupyterutils-0.17.0 pykerberos-1.2.1 requests-kerberos-0.12.0 sparkmagic-0.17.0\nCollecting pyspark\n  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n\u001b[K     |████████████████████████████████| 204.2 MB 27 kB/s s eta 0:00:01\n\u001b[?25hCollecting py4j==0.10.9\n  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n\u001b[K     |████████████████████████████████| 198 kB 41.5 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612244 sha256=01f159ca4876b42f560476335ee143f4677b40c244ea5210e7b3683263793aab\n  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.9 pyspark-3.0.1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.types import * #Import spark types\nfrom pyspark.sql.functions import * #Import spark functions\n\nimport pyspark\nfrom pyspark.sql import SparkSession #Import the spark session\nfrom pyspark import SparkContext #Create a spark context\nfrom pyspark.sql import SQLContext #Create an SQL context\n\nfrom pyspark.ml.feature import Tokenizer #Used to tokenize the tweet data\nfrom pyspark.ml.feature import CountVectorizer #Used to make the data into vectors\nfrom pyspark.ml import Pipeline #Build a pipeline\nfrom pyspark.ml.classification import RandomForestClassifier #The chosen classifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator #Metrics\n\nconf = pyspark.SparkConf().setAll([('spark.executor.memory', '16g'), ('spark.executor.cores', '1'), ('spark.cores.max', '1'), ('spark.driver.memory','16g')])\nsc = SparkContext.getOrCreate(conf = conf) #Initialize the spark context\nsqlContext = SQLContext.getOrCreate(sc) #Create an SQL Context\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate() #Make a spark session","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = spark.read.csv(\"../input/sentiment140/training.1600000.processed.noemoticon.csv\", inferSchema = True, header = False) #Read in the data\ntweets.show(10) #Show the first 10 columns","execution_count":4,"outputs":[{"output_type":"stream","text":"+---+----------+--------------------+--------+---------------+--------------------+\n|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n+---+----------+--------------------+--------+---------------+--------------------+\n|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n|  0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n|  0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n|  0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n|  0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n|  0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n+---+----------+--------------------+--------+---------------+--------------------+\nonly showing top 10 rows\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"tweet\", \"cleanTweet\"] #Column names\n\n#Fix column names, as this dataset did not have a header\ntweets = tweets.select(col(\"_c0\").alias(labels[0]), col(\"_c1\").alias(labels[1]), col(\"_c2\").alias(labels[2]),\n                      col(\"_c3\").alias(labels[3]), col(\"_c4\").alias(labels[4]), col(\"_c5\").alias(labels[5]))\ntweets.show(10) #Show the dataset","execution_count":5,"outputs":[{"output_type":"stream","text":"+------+----------+--------------------+--------+---------------+--------------------+\n|target|        id|                date|    flag|           user|               tweet|\n+------+----------+--------------------+--------+---------------+--------------------+\n|     0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n|     0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n|     0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n|     0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n|     0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n|     0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n|     0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n|     0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n|     0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n|     0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n+------+----------+--------------------+--------+---------------+--------------------+\nonly showing top 10 rows\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Check for Null Values"},{"metadata":{},"cell_type":"markdown","source":"Source: https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"#For each column, count cases where the column is NaN or Null\ntweets.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in tweets.columns]).show() #Check for null values","execution_count":6,"outputs":[{"output_type":"stream","text":"+------+---+----+----+----+-----+\n|target| id|date|flag|user|tweet|\n+------+---+----+----+----+-----+\n|     0|  0|   0|   0|   0|    0|\n+------+---+----+----+----+-----+\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"There are no null values."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Sentiment Counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.groupBy(\"target\").count().orderBy(\"count\").show() #Check how many of each target value there is","execution_count":7,"outputs":[{"output_type":"stream","text":"+------+------+\n|target| count|\n+------+------+\n|     0|800000|\n|     4|800000|\n+------+------+\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"There are an equal number of positive (4) and negative (0) tweets in the dataset. There are no neutral values"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Fix the Sentiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change tweet sentiment to 0 and 1 instead of 0 and 4\ntweets = tweets.withColumn(\"target\", \\\n              when(tweets[\"target\"] == 4, 1).otherwise(tweets[\"target\"]))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.groupBy(\"target\").count().orderBy(\"count\").show() #Check how many of each target value there is","execution_count":9,"outputs":[{"output_type":"stream","text":"+------+------+\n|target| count|\n+------+------+\n|     1|800000|\n|     0|800000|\n+------+------+\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Clean the Tweets"},{"metadata":{},"cell_type":"markdown","source":"Reusing my clean tweets code from my Coronavirus tweet analysis: https://www.kaggle.com/lunamcbride24/coronavirus-tweet-processing\nSource on adding the new column: https://stackoverflow.com/questions/48164206/pyspark-adding-a-column-from-a-list-of-values-using-a-udf"},{"metadata":{"trusted":true},"cell_type":"code","source":"punctuations = \"\"\"!()-![]{};:+'\"\\,<>./?@#$%^&*_~Â\"\"\" #List of punctuations to remove, including a weird A that will not process out any other way\n\n#CleanTweets: parces the tweets and removes punctuation, stop words, digits, and links.\n#Input: the list of tweets that need parsing\n#Output: the parsed tweets\ndef cleanTweets(tweetParse):\n    length = len(tweetParse)\n    for i in range(0,length):\n        tweet = tweetParse[i] #Putting the tweet into a variable so that it is not calling tweetParse[i] over and over\n        tweet = html.unescape(tweet) #Removes leftover HTML elements, such as &amp;\n        tweet = re.sub(r\"@\\w+\", ' ', tweet) #Completely removes @'s, as other peoples' usernames mean nothing\n        tweet = re.sub(r'http\\S+', ' ', tweet) #Removes links, as links provide no data in tweet analysis in themselves\n        tweet = re.sub(r\"\\d+\\S+\", ' ', tweet) #Removes numbers, as well as cases like the \"th\" in \"14th\"\n        tweet = ''.join([punc for punc in tweet if not punc in punctuations]) #Removes the punctuation defined above\n        tweet = tweet.lower() #Turning the tweets lowercase real quick for later use\n    \n        tweetWord = tweet.split() #Splits the tweet into individual words\n        tweetParse[i] = ''.join([word + \" \" for word in tweetWord if nlp.vocab[word].is_stop == False]) #Checks if the words are stop words\n        \n    return tweetParse #Returns the parsed tweets","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweetText = tweets.select(\"tweet\").collect() #Collect the tweet data\ntweetText = [str(tweet.tweet) for tweet in tweetText] #Make the tweets strings\ncleanTweet = cleanTweets(tweetText) #Clean the tweets","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Connect Clean Tweets with Sentiments"},{"metadata":{},"cell_type":"markdown","source":"## Collect Sentiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment = tweets.select(\"target\").collect() #Collect the sentiment data\nsentiment = [int(tweet.target) for tweet in sentiment] #Make the sentiments into integers\nprint(sentiment[:10]) #Look at the list of sentiments","execution_count":12,"outputs":[{"output_type":"stream","text":"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Connect the Tweets and Sentiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweetSentiment = [] #A list to hold the combined tweet-sentiment pair\nlengthClean = len(cleanTweet) #Get the length for looping\n\n#For loop to combine clean tweet with its sentiment\nfor index in range(0, lengthClean):\n    tweetSentiment.append((cleanTweet[index], sentiment[index])) #Add the combined pair to the tweetSentiment list\n\nprint(tweetSentiment[:10]) #Print a couple of the pairings","execution_count":13,"outputs":[{"output_type":"stream","text":"[('awww thats bummer shoulda got david carr day d ', 0), ('upset cant update facebook texting cry result school today blah ', 0), ('dived times ball managed save rest bounds ', 0), ('body feels itchy like fire ', 0), ('behaving im mad cant ', 0), ('crew ', 0), ('need hug ', 0), ('hey long time yes rains bit bit lol im fine thanks hows ', 0), ('nope didnt ', 0), ('que muera ', 0)]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Put into a PySpark Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweetColumns = [\"cleanTweet\",\"trueTarget\"] #Put the column names we want to use\nsentDF = spark.createDataFrame(data = tweetSentiment, schema = tweetColumns) #Put the data into a new dataframe\nsentDF.cache() #Cashe the dataset\nsentDF.show(10) #Take a peek at the new dataset","execution_count":14,"outputs":[{"output_type":"stream","text":"+--------------------+----------+\n|          cleanTweet|trueTarget|\n+--------------------+----------+\n|awww thats bummer...|         0|\n|upset cant update...|         0|\n|dived times ball ...|         0|\n|body feels itchy ...|         0|\n|behaving im mad c...|         0|\n|               crew |         0|\n|           need hug |         0|\n|hey long time yes...|         0|\n|         nope didnt |         0|\n|          que muera |         0|\n+--------------------+----------+\nonly showing top 10 rows\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Fix cleanTweet Nulls"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentDF.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in sentDF.columns]).show() #Check for null values","execution_count":15,"outputs":[{"output_type":"stream","text":"+----------+----------+\n|cleanTweet|trueTarget|\n+----------+----------+\n|         1|         0|\n+----------+----------+\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Take a peek at the non-null counts\nprint(\"Tweet: \", sentDF.filter(isnan(sentDF.cleanTweet) == False).count(), \"\\nTarget: \", sentDF.filter(isnan(sentDF.trueTarget) == False).count())","execution_count":16,"outputs":[{"output_type":"stream","text":"Tweet:  1599999 \nTarget:  1600000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentDF = sentDF.where(isnan(sentDF.cleanTweet) == False) #Remove the null value\nsentDF.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in sentDF.columns]).show() #Check for null values","execution_count":17,"outputs":[{"output_type":"stream","text":"+----------+----------+\n|cleanTweet|trueTarget|\n+----------+----------+\n|         0|         0|\n+----------+----------+\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Train-Test Split the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Test = sentDF.randomSplit([0.75, 0.25]) #Split the data 75-25\nsentTrain = Train_Test[0] #Put the train data into its own variable\nsentTest = Train_Test[1] #Put the test data into its own variable\nprint(\"Train: \", sentTrain.count(), \"\\nTest: \", sentTest.count()) #Print split numbers","execution_count":18,"outputs":[{"output_type":"stream","text":"Train:  1199942 \nTest:  400057\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Build Pipeline to Tokenize/Vectorize the Tweets"},{"metadata":{},"cell_type":"markdown","source":"Source: https://classes.ischool.syr.edu/ist718/content/unit09/lab-sentiment_analysis/"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer().setInputCol(\"cleanTweet\").setOutputCol(\"tokenTweet\") #Build the tokenizer\nvectorizer = CountVectorizer().setInputCol(\"tokenTweet\").setOutputCol(\"features\") #Build the vectors\nforest = RandomForestClassifier(labelCol = \"trueTarget\", featuresCol=\"features\", numTrees = 3, maxDepth = 16) #Build the forest classifier\nmlPipe = Pipeline(stages = [tokenizer, vectorizer, forest]) #Build the pipeline to do all above steps","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Build the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = mlPipe.fit(sentTrain) #Fit the data","execution_count":20,"outputs":[{"output_type":"stream","text":"----------------------------------------\nException happened during processing of request from ('127.0.0.1', 44184)\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/socketserver.py\", line 316, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"/opt/conda/lib/python3.7/socketserver.py\", line 347, in process_request\n    self.finish_request(request, client_address)\n  File \"/opt/conda/lib/python3.7/socketserver.py\", line 360, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/opt/conda/lib/python3.7/socketserver.py\", line 720, in __init__\n    self.handle()\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/accumulators.py\", line 268, in handle\n    poll(accum_updates)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/accumulators.py\", line 241, in poll\n    if func():\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/accumulators.py\", line 245, in accum_updates\n    num_updates = read_int(self.rfile)\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/serializers.py\", line 595, in read_int\n    raise EOFError\nEOFError\n----------------------------------------\n","name":"stderr"},{"output_type":"error","ename":"Py4JError","evalue":"An error occurred while calling o216.fit","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-79518ce8398f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlPipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentTrain\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Fit the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o216.fit"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions = model.transform(sentTest) #Predict the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions.show(5) #Show the prediction table","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}