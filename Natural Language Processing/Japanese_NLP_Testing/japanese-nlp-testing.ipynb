{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Natural Language Processing Project: Japanese ML Tweets"},{"metadata":{},"cell_type":"markdown","source":"Coded by Luna McBride\n\nThe point of this notebook is to test Japanese language processing."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys #Used exclusively to get ja_core_news_lg, which does not save between sessions\n!{sys.executable} -m spacy download ja_core_news_lg #Downloads ja_core_news_lg, which is used for spacy Japanese processing","execution_count":23,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: ja_core_news_lg==2.3.2 from https://github.com/explosion/spacy-models/releases/download/ja_core_news_lg-2.3.2/ja_core_news_lg-2.3.2.tar.gz#egg=ja_core_news_lg==2.3.2 in /opt/conda/lib/python3.7/site-packages (2.3.2)\nRequirement already satisfied: sudachidict-core>=20200330 in /opt/conda/lib/python3.7/site-packages (from ja_core_news_lg==2.3.2) (20200722)\nRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from ja_core_news_lg==2.3.2) (2.3.2)\nRequirement already satisfied: sudachipy>=0.4.5 in /opt/conda/lib/python3.7/site-packages (from ja_core_news_lg==2.3.2) (0.4.9)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (2.23.0)\nRequirement already satisfied: thinc==7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (7.4.1)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (1.0.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (2.0.3)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (0.4.1)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (1.1.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (46.1.3.post20200325)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (1.0.2)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (3.0.2)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (1.0.0)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (0.8.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (4.45.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (1.18.5)\nRequirement already satisfied: sortedcontainers~=2.1.0 in /opt/conda/lib/python3.7/site-packages (from sudachipy>=0.4.5->ja_core_news_lg==2.3.2) (2.1.0)\nRequirement already satisfied: dartsclone~=0.9.0 in /opt/conda/lib/python3.7/site-packages (from sudachipy>=0.4.5->ja_core_news_lg==2.3.2) (0.9.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (2020.6.20)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (1.24.3)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (1.6.0)\nRequirement already satisfied: Cython in /opt/conda/lib/python3.7/site-packages (from dartsclone~=0.9.0->sudachipy>=0.4.5->ja_core_news_lg==2.3.2) (0.29.21)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->ja_core_news_lg==2.3.2) (3.1.0)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('ja_core_news_lg')\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # regular expressions\nimport html # HTML content, like &amp;\n\nimport spacy #For general NLP\nfrom spacy.lang.ja.stop_words import STOP_WORDS #Get the Japanese stopwords\n\nimport ja_core_news_lg #Japanese language handling\nnlp =  ja_core_news_lg.load() #Initializing Spacy for Japanese\nimport operator #For dictionary sorting\n\nimport matplotlib.pylab as plt #For plot testing\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":24,"outputs":[{"output_type":"stream","text":"/kaggle/input/ml-tweet/Twitter ML Data.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Load the dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tweetData = pd.read_csv(\"../input/ml-tweet/Twitter ML Data.csv\") #Load the dataset into pandas\ntweetData.head() #Take a peek at the dataset","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"      Name                                                  2  \\\n0      NaN                                                NaN   \n1      NaN                                                NaN   \n2      NaN                                                NaN   \n3  aketaco  RT @odashi_t: 「機械学習がずるをしている（人間が想定しない推論を行う）から良く...   \n4  Atsunov  RT @ML_deep: y = f(x)\\n\\nというモデルで、ひと目では分からぬような関...   \n\n                                                   3  \\\n0                                                NaN   \n1                                                NaN   \n2                                                NaN   \n3  https://twitter.com/187400312/status/127142474...   \n4  https://twitter.com/14404737/status/1271428080...   \n\n                                4   5   6  \n0                             NaN NaN NaN  \n1                             NaN NaN NaN  \n2                             NaN NaN NaN  \n3  Fri Jun 12 12:50:56 +0000 2020 NaN NaN  \n4  Fri Jun 12 13:04:10 +0000 2020 NaN NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aketaco</td>\n      <td>RT @odashi_t: 「機械学習がずるをしている（人間が想定しない推論を行う）から良く...</td>\n      <td>https://twitter.com/187400312/status/127142474...</td>\n      <td>Fri Jun 12 12:50:56 +0000 2020</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Atsunov</td>\n      <td>RT @ML_deep: y = f(x)\\n\\nというモデルで、ひと目では分からぬような関...</td>\n      <td>https://twitter.com/14404737/status/1271428080...</td>\n      <td>Fri Jun 12 13:04:10 +0000 2020</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Add column names"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweetData = tweetData.rename(columns = {\"Name\" : \"Tweeter\", \"2\" : \"OriginalTweet\", \"3\" : \"TweetLink\", \"4\" : \"TweetTime\"}) #Changes the column names to something more helpful\ntweetData.head() #Take a peek at the dataset","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"   Tweeter                                      OriginalTweet  \\\n0      NaN                                                NaN   \n1      NaN                                                NaN   \n2      NaN                                                NaN   \n3  aketaco  RT @odashi_t: 「機械学習がずるをしている（人間が想定しない推論を行う）から良く...   \n4  Atsunov  RT @ML_deep: y = f(x)\\n\\nというモデルで、ひと目では分からぬような関...   \n\n                                           TweetLink  \\\n0                                                NaN   \n1                                                NaN   \n2                                                NaN   \n3  https://twitter.com/187400312/status/127142474...   \n4  https://twitter.com/14404737/status/1271428080...   \n\n                        TweetTime   5   6  \n0                             NaN NaN NaN  \n1                             NaN NaN NaN  \n2                             NaN NaN NaN  \n3  Fri Jun 12 12:50:56 +0000 2020 NaN NaN  \n4  Fri Jun 12 13:04:10 +0000 2020 NaN NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweeter</th>\n      <th>OriginalTweet</th>\n      <th>TweetLink</th>\n      <th>TweetTime</th>\n      <th>5</th>\n      <th>6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aketaco</td>\n      <td>RT @odashi_t: 「機械学習がずるをしている（人間が想定しない推論を行う）から良く...</td>\n      <td>https://twitter.com/187400312/status/127142474...</td>\n      <td>Fri Jun 12 12:50:56 +0000 2020</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Atsunov</td>\n      <td>RT @ML_deep: y = f(x)\\n\\nというモデルで、ひと目では分からぬような関...</td>\n      <td>https://twitter.com/14404737/status/1271428080...</td>\n      <td>Fri Jun 12 13:04:10 +0000 2020</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Check for and remove null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tweetData.count()) #Get the counts of values in the dataset\n\n#Check each column if there are any null values\nprint(tweetData[\"Tweeter\"].isnull().any())\nprint(tweetData[\"OriginalTweet\"].isnull().any())\nprint(tweetData[\"TweetLink\"].isnull().any())\nprint(tweetData[\"TweetTime\"].isnull().any())\nprint(tweetData[\"5\"].isnull().any())\nprint(tweetData[\"6\"].isnull().any())","execution_count":6,"outputs":[{"output_type":"stream","text":"Tweeter          1390\nOriginalTweet    1390\nTweetLink        1390\nTweetTime        1390\n5                   0\n6                   0\ndtype: int64\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"It appears there are no values at all in 5 and 6, so those should be removed entirely. There are also null rows, as shown in the head functions above, so I will just drop those with null tweets and see where to go from there"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweetData = tweetData.drop(columns = [\"5\", \"6\"]) #Drop the null columns\ntweetData.head() #Take a peek at the dataset","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"   Tweeter                                      OriginalTweet  \\\n0      NaN                                                NaN   \n1      NaN                                                NaN   \n2      NaN                                                NaN   \n3  aketaco  RT @odashi_t: 「機械学習がずるをしている（人間が想定しない推論を行う）から良く...   \n4  Atsunov  RT @ML_deep: y = f(x)\\n\\nというモデルで、ひと目では分からぬような関...   \n\n                                           TweetLink  \\\n0                                                NaN   \n1                                                NaN   \n2                                                NaN   \n3  https://twitter.com/187400312/status/127142474...   \n4  https://twitter.com/14404737/status/1271428080...   \n\n                        TweetTime  \n0                             NaN  \n1                             NaN  \n2                             NaN  \n3  Fri Jun 12 12:50:56 +0000 2020  \n4  Fri Jun 12 13:04:10 +0000 2020  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweeter</th>\n      <th>OriginalTweet</th>\n      <th>TweetLink</th>\n      <th>TweetTime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aketaco</td>\n      <td>RT @odashi_t: 「機械学習がずるをしている（人間が想定しない推論を行う）から良く...</td>\n      <td>https://twitter.com/187400312/status/127142474...</td>\n      <td>Fri Jun 12 12:50:56 +0000 2020</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Atsunov</td>\n      <td>RT @ML_deep: y = f(x)\\n\\nというモデルで、ひと目では分からぬような関...</td>\n      <td>https://twitter.com/14404737/status/1271428080...</td>\n      <td>Fri Jun 12 13:04:10 +0000 2020</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweetData[\"OriginalTweet\"] = tweetData[\"OriginalTweet\"].apply(lambda x: np.nan if not x else x) #Change blank tweets to null as well\ntweetData.dropna(subset = [\"OriginalTweet\"], inplace = True) #Drop the nulls based on the tweet data\ntweetData.reset_index(drop=True, inplace=True) #Reset the index for later looping\ntweetData.head() #Take a peek at the dataset","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"           Tweeter                                      OriginalTweet  \\\n0          aketaco  RT @odashi_t: 「機械学習がずるをしている（人間が想定しない推論を行う）から良く...   \n1          Atsunov  RT @ML_deep: y = f(x)\\n\\nというモデルで、ひと目では分からぬような関...   \n2    arlequin_udon  この話をしていたら\\n「その日記データを機械学習に食わせたら、勝手に日記書きだすんじゃね？」...   \n3      daruchan524  RT @hokuto_sd: JSAI2020の社会データと予測のセッションで、共同研究をし...   \n4  usdatascientist  RT @unistud_ml: @usdatascientist 購入しました!昨日から見て...   \n\n                                           TweetLink  \\\n0  https://twitter.com/187400312/status/127142474...   \n1  https://twitter.com/14404737/status/1271428080...   \n2  https://twitter.com/177861086/status/127142784...   \n3  https://twitter.com/1879931462/status/12714277...   \n4  https://twitter.com/1186121103992737792/status...   \n\n                        TweetTime  \n0  Fri Jun 12 12:50:56 +0000 2020  \n1  Fri Jun 12 13:04:10 +0000 2020  \n2  Fri Jun 12 13:03:14 +0000 2020  \n3  Fri Jun 12 13:02:41 +0000 2020  \n4  Fri Jun 12 13:02:24 +0000 2020  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweeter</th>\n      <th>OriginalTweet</th>\n      <th>TweetLink</th>\n      <th>TweetTime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aketaco</td>\n      <td>RT @odashi_t: 「機械学習がずるをしている（人間が想定しない推論を行う）から良く...</td>\n      <td>https://twitter.com/187400312/status/127142474...</td>\n      <td>Fri Jun 12 12:50:56 +0000 2020</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Atsunov</td>\n      <td>RT @ML_deep: y = f(x)\\n\\nというモデルで、ひと目では分からぬような関...</td>\n      <td>https://twitter.com/14404737/status/1271428080...</td>\n      <td>Fri Jun 12 13:04:10 +0000 2020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>arlequin_udon</td>\n      <td>この話をしていたら\\n「その日記データを機械学習に食わせたら、勝手に日記書きだすんじゃね？」...</td>\n      <td>https://twitter.com/177861086/status/127142784...</td>\n      <td>Fri Jun 12 13:03:14 +0000 2020</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>daruchan524</td>\n      <td>RT @hokuto_sd: JSAI2020の社会データと予測のセッションで、共同研究をし...</td>\n      <td>https://twitter.com/1879931462/status/12714277...</td>\n      <td>Fri Jun 12 13:02:41 +0000 2020</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>usdatascientist</td>\n      <td>RT @unistud_ml: @usdatascientist 購入しました!昨日から見て...</td>\n      <td>https://twitter.com/1186121103992737792/status...</td>\n      <td>Fri Jun 12 13:02:24 +0000 2020</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check each column if there are any null values\nprint(tweetData[\"Tweeter\"].isnull().any())\nprint(tweetData[\"OriginalTweet\"].isnull().any())\nprint(tweetData[\"TweetLink\"].isnull().any())\nprint(tweetData[\"TweetTime\"].isnull().any())","execution_count":9,"outputs":[{"output_type":"stream","text":"False\nFalse\nFalse\nFalse\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"That took out all the null values. "},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Tweet cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modified from my previous nlp project: https://www.kaggle.com/lunamcbride24/coronavirus-tweet-processing\n\npunctuations = \"\"\"!()（）「」、-!！[]{};:+'\"\\,<>./?@#$%^&*_~Â。…・，【】？\"\"\" #List of punctuations to remove, including a weird A that will not process out any other way\n\nstopwords = spacy.lang.ja.stop_words.STOP_WORDS\n\n#CleanTweets: parces the tweets and removes punctuation, stop words, digits, and links.\n#Input: the list of tweets that need parsing\n#Output: the parsed tweets\ndef cleanTweets(tweetParse):\n    for i in range(0,len(tweetParse)):\n        tweet = tweetParse[i] #Putting the tweet into a variable so that it is not calling tweetParse[i] over and over\n        tweet = html.unescape(tweet) #Removes leftover HTML elements, such as &amp;\n        tweet = re.sub(r\"RT\", ' ', tweet)\n        tweet = re.sub(r\"\\n\", ' ', tweet)\n        tweet = re.sub(r\"@\\w+\", ' ', tweet) #Completely removes @'s, as other peoples' usernames mean nothing\n        tweet = re.sub(r'https\\S+', ' ', tweet) #Removes links, as links provide no data in tweet analysis in themselves\n        tweet = re.sub(r\"\\d+\", ' ', tweet) #Removes numbers, as well as cases like the \"th\" in \"14th\"\n        tweet = ''.join([punc for punc in tweet if not punc in punctuations]) #Removes the punctuation defined above\n        tweet = tweet.lower() #Turning the tweets lowercase real quick for later use\n    \n        tweetWord = nlp.tokenizer(tweet) #Splits the tweet into individual words\n        tweetParse[i] = ''.join([word.orth_ + \" \" for word in tweetWord if word.is_stop == False]) #Checks if the words are stop words\n       \n        \n    return tweetParse #Returns the parsed tweets\n\ntweets = tweetData[\"OriginalTweet\"].copy() #Gets a copy of the tweets to send to the function call\ntweetData[\"CleanTweet\"] = cleanTweets(tweets) #Adds a CleanTweet column and fills it with processed tweets\nprint(tweetData[\"OriginalTweet\"][3], \"\\n \\n\", tweetData[\"CleanTweet\"][3]) #Prints an example sentence\ntweetData.head() #Takes a peek at the dataframe","execution_count":10,"outputs":[{"output_type":"stream","text":"RT @hokuto_sd: JSAI2020の社会データと予測のセッションで、共同研究をしている桑田和さんが「機械学習アプローチに基づく中古ファッションアイテムの価格保持期間の適正化モデルの提案」というタイトルで発表しました！\n#JSAI2020 \nhttps://t.co/… \n \n      jsai 社会 データ 予測 セッション 共同 研究 桑田 和 機械 学習 アプローチ 基づく 中古 ファッション アイテム 価格 保持 期間 適正 化 モデル 提案 タイトル 発表 まし jsai     \n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"           Tweeter                                      OriginalTweet  \\\n0          aketaco  RT @odashi_t: 「機械学習がずるをしている（人間が想定しない推論を行う）から良く...   \n1          Atsunov  RT @ML_deep: y = f(x)\\n\\nというモデルで、ひと目では分からぬような関...   \n2    arlequin_udon  この話をしていたら\\n「その日記データを機械学習に食わせたら、勝手に日記書きだすんじゃね？」...   \n3      daruchan524  RT @hokuto_sd: JSAI2020の社会データと予測のセッションで、共同研究をし...   \n4  usdatascientist  RT @unistud_ml: @usdatascientist 購入しました!昨日から見て...   \n\n                                           TweetLink  \\\n0  https://twitter.com/187400312/status/127142474...   \n1  https://twitter.com/14404737/status/1271428080...   \n2  https://twitter.com/177861086/status/127142784...   \n3  https://twitter.com/1879931462/status/12714277...   \n4  https://twitter.com/1186121103992737792/status...   \n\n                        TweetTime  \\\n0  Fri Jun 12 12:50:56 +0000 2020   \n1  Fri Jun 12 13:04:10 +0000 2020   \n2  Fri Jun 12 13:03:14 +0000 2020   \n3  Fri Jun 12 13:02:41 +0000 2020   \n4  Fri Jun 12 13:02:24 +0000 2020   \n\n                                          CleanTweet  \n0       機械 学習 ずる 人間 想定 推論 行う 良く 思う 人 人間 理解 推論 行う ...  \n1       y = fx   モデル ひと目 分から 関係 性 大量 y x 求めよう 機械 ...  \n2  話 日記 データ 機械 学習 食わ 勝手に 日記 書き だす じゃ って 言わ 笑っ なに ...  \n3       jsai 社会 データ 予測 セッション 共同 研究 桑田 和 機械 学習 アプロ...  \n4           購入 まし 昨日 見 すごく わかり やすい 機械 学習 使える 頑張っ 勉強   ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweeter</th>\n      <th>OriginalTweet</th>\n      <th>TweetLink</th>\n      <th>TweetTime</th>\n      <th>CleanTweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aketaco</td>\n      <td>RT @odashi_t: 「機械学習がずるをしている（人間が想定しない推論を行う）から良く...</td>\n      <td>https://twitter.com/187400312/status/127142474...</td>\n      <td>Fri Jun 12 12:50:56 +0000 2020</td>\n      <td>機械 学習 ずる 人間 想定 推論 行う 良く 思う 人 人間 理解 推論 行う ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Atsunov</td>\n      <td>RT @ML_deep: y = f(x)\\n\\nというモデルで、ひと目では分からぬような関...</td>\n      <td>https://twitter.com/14404737/status/1271428080...</td>\n      <td>Fri Jun 12 13:04:10 +0000 2020</td>\n      <td>y = fx   モデル ひと目 分から 関係 性 大量 y x 求めよう 機械 ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>arlequin_udon</td>\n      <td>この話をしていたら\\n「その日記データを機械学習に食わせたら、勝手に日記書きだすんじゃね？」...</td>\n      <td>https://twitter.com/177861086/status/127142784...</td>\n      <td>Fri Jun 12 13:03:14 +0000 2020</td>\n      <td>話 日記 データ 機械 学習 食わ 勝手に 日記 書き だす じゃ って 言わ 笑っ なに ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>daruchan524</td>\n      <td>RT @hokuto_sd: JSAI2020の社会データと予測のセッションで、共同研究をし...</td>\n      <td>https://twitter.com/1879931462/status/12714277...</td>\n      <td>Fri Jun 12 13:02:41 +0000 2020</td>\n      <td>jsai 社会 データ 予測 セッション 共同 研究 桑田 和 機械 学習 アプロ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>usdatascientist</td>\n      <td>RT @unistud_ml: @usdatascientist 購入しました!昨日から見て...</td>\n      <td>https://twitter.com/1186121103992737792/status...</td>\n      <td>Fri Jun 12 13:02:24 +0000 2020</td>\n      <td>購入 まし 昨日 見 すごく わかり やすい 機械 学習 使える 頑張っ 勉強</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Japanese Stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(stopwords)) #Print the stopwords for reference","execution_count":11,"outputs":[{"output_type":"stream","text":"['いずれ', 'に', 'ば', 'ほど', 'それぞれ', 'すぐ', 'ため', 'でき', 'なし', 'れ', 'いく', 'ほとんど', 'そこ', 'れる', 'なく', 'ない', 'ご', 'す', 'なお', 'ある', 'よう', 'ら', 'かつ', 'たら', 'なっ', 'しかし', 'いう', 'いる', 'そう', 'た', 'の', 'そして', 'できる', 'その', 'せる', 'しまっ', 'あれ', 'かなり', 'まま', 'き', 'み', 'も', 'から', 'かけ', 'なり', 'ここ', '一', 'どう', 'おり', 'か', 'へ', 'ます', 'にて', 'いっ', 'ながら', 'また', 'つつ', 'それ', 'が', 'ね', 'よる', 'たり', 'る', 'しまう', 'あまり', 'この', 'たち', 'い', 'なかっ', 'のみ', 'これ', 'つい', 'など', 'する', 'いわ', 'しよう', 'おけ', 'とも', 'よ', 'ごと', 'および', 'あり', 'より', 'あ', 'や', 'お', 'たい', 'ところ', 'は', 'られる', 'はじめ', 'もっ', 'しか', 'ひと', 'だっ', 'とっ', 'せい', 'のち', 'くる', 'らしい', 'で', 'し', 'まで', 'こう', 'すべて', 'あるいは', 'さ', 'ず', 'です', 'な', 'もう', 'なる', 'うち', 'よれ', 'ほぼ', 'もと', 'やっ', 'おい', 'ぶり', 'なら', 'こ', 'いい', 'ぬ', 'いつ', 'ただし', 'だ', 'べき', 'ん', 'おら', 'だけ', 'ほか', 'ちゃん', 'つけ', 'なけれ', 'え', 'て', 'られ', 'かつて', 'ま', 'と', 'とき', 'よっ', 'もの', 'くん', 'よく', 'つ', 'きっかけ', 'せ', 'こと', 'を', 'あっ', 'さん', 'ち', 'さらに']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Get word counts"},{"metadata":{},"cell_type":"markdown","source":"Quick source list: https://www.geeksforgeeks.org/python-get-first-n-keyvalue-pairs-in-given-dictionary/ , https://www.w3resource.com/python-exercises/dictionary/python-data-type-dictionary-exercise-1.php "},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = tweetData[\"CleanTweet\"].copy() #Copy the clean tweets for processing\ncount = dict() #Creates a dictionary \nfor i in range(0,len(tweets)):\n    words = tweets[i].split()\n    for word in words:\n        if word in count:\n            count[word] += 1\n        else:\n            count[word] = 1\n\nsortCount = {word : summ for word, summ in sorted(count.items(), key=operator.itemgetter(1),reverse=True)}\nprint(dict(list(sortCount.items())[0: 20]))","execution_count":12,"outputs":[{"output_type":"stream","text":"{'学習': 1783, '機械': 1589, 'ai': 243, 'データ': 235, '的': 223, 'けど': 210, 'python': 193, '中国': 188, 'てる': 178, '性': 163, '人': 161, '世界': 156, '人工': 148, '知能': 147, 'って': 145, '第': 140, '可能': 139, '年': 133, 'プログラミング': 129, '次': 126}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, there are several top options here that I would consider stopwords, but are not in the stopwords list. For example, けど, which means but, but is also used in sentences to imply another sentence (which is not something a machine can likely pick up on). The other words I would consider stopwords in the top 20 would be 的, てる, って, 第, and 次."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Matplotlib Japanese Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"top = dict(list(sortCount.items())[0: 20])\nfig, axes = plt.subplots(nrows = 1, ncols = 1, figsize=(16,8))\nplt.bar(top.keys(), top.values(), color = \"g\")","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"<BarContainer object of 20 artists>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1152x576 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA6sAAAHSCAYAAAAKUF2lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAayElEQVR4nO3df6zdd33f8debuE3TtSkwG5TaiZyiwBaizShWBGOgVLRLQFMDm9gcaZAVOgOCbqj9A9JOSmCKNK0wNLSRyowUskKydCkjrQI0RRusFRQc6uYXpDgka4yjxAzWssG8Jbz3x/2anjjXP3Lv8b2fc/14SEf33M/5fs953+t7wc98v+fr6u4AAADASJ6x3gMAAADA0cQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMPZtN4DnMjmzZt7+/bt6z0GAAAAc7Z58+Z8+tOf/nR3X370Y8PH6vbt27N37971HgMAAIBToKo2L7fuNGAAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABiOWAUAAGA4YhUAAIDhiFUAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDib1nuARVfvqvUeIX1Nr/cIAAAAc+XIKgAAAMMRqwAAAAxHrAIAADCcE8ZqVd1QVY9V1T0za/+xqvZNt4eqat+0vr2qvjfz2K/P7HNxVd1dVfur6v1Vtf5v9gQAAGBIJ3OBpQ8n+bdJbjyy0N3/8Mj9qnpvkj+f2f6B7t6xzPNcn2R3ki8kuT3J5Uk++fRHBgAAYKM74ZHV7v5ckm8t99h0dPQfJLnpeM9RVeckObu7P9/dnaXwffXTHxcAAIDTwWrfs/qyJI9299dm1s6vqj+uqs9W1cumta1JDsxsc2BaW1ZV7a6qvVW199ChQ6scEQAAgEWz2li9Mk8+qvpIkvO6+0VJfinJx6rq7CTLvT/1mP84aHfv6e6d3b1zy5YtqxwRAACARXMy71ldVlVtSvL3klx8ZK27Dyc5PN2/s6oeSPL8LB1J3Taz+7YkB1f62gAAAGxsqzmy+jNJvtrdPzi9t6q2VNUZ0/2fSnJBkq939yNJvlNVL57e5/r6JJ9YxWsDAACwgZ3MP11zU5LPJ3lBVR2oqjdOD+3KUy+s9PIkd1XVnyT5T0ne3N1HLs70liT/Psn+JA/ElYABAAA4hhOeBtzdVx5j/R8vs3ZrkluPsf3eJBc9zfkAAAA4Da32AksAAAAwd2IVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABiOWAUAAGA4YhUAAIDhiFUAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABiOWAUAAGA4YhUAAIDhiFUAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABjOpvUegFOv3lXrPUL6ml7vEQAAgAXiyCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMMRqwAAAAxHrAIAADAcsQoAAMBwxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcE4Yq1V1Q1U9VlX3zKxdW1XfqKp90+1VM49dXVX7q+r+qrpsZv3iqrp7euz9VVXz/3IAAADYCE7myOqHk1y+zPr7unvHdLs9SarqwiS7krxw2ucDVXXGtP31SXYnuWC6LfecAAAAcOJY7e7PJfnWST7fFUlu7u7D3f1gkv1JLqmqc5Kc3d2f7+5OcmOSV690aAAAADa21bxn9W1Vddd0mvCzprWtSR6e2ebAtLZ1un/0+rKqandV7a2qvYcOHVrFiAAAACyilcbq9Umel2RHkkeSvHdaX+59qH2c9WV1957u3tndO7ds2bLCEQEAAFhUK4rV7n60u5/o7u8n+WCSS6aHDiQ5d2bTbUkOTuvbllkHAACAp1hRrE7vQT3iNUmOXCn4tiS7qurMqjo/SxdS+mJ3P5LkO1X14ukqwK9P8olVzA0AAMAGtulEG1TVTUkuTbK5qg4kuSbJpVW1I0un8j6U5E1J0t33VtUtSe5L8niSt3b3E9NTvSVLVxY+K8knpxsAAAA8xQljtbuvXGb5Q8fZ/rok1y2zvjfJRU9rOgAAAE5Lq7kaMAAAAJwSYhUAAIDhiFUAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABiOWAUAAGA4YhUAAIDhiFUAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABiOWAUAAGA4YhUAAIDhiFUAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABiOWAUAAGA4YhUAAIDhiFUAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABiOWAUAAGA4J4zVqrqhqh6rqntm1n6tqr5aVXdV1cer6pnT+vaq+l5V7Ztuvz6zz8VVdXdV7a+q91dVnZovCQAAgEV3MkdWP5zk8qPW7khyUXf/jSR/muTqmcce6O4d0+3NM+vXJ9md5ILpdvRzAgAAQJKTiNXu/lySbx219nvd/fj06ReSbDvec1TVOUnO7u7Pd3cnuTHJq1c2MgAAABvdPN6z+oYkn5z5/Pyq+uOq+mxVvWxa25rkwMw2B6a1ZVXV7qraW1V7Dx06NIcRAQAAWCSritWq+tUkjyf56LT0SJLzuvtFSX4pyceq6uwky70/tY/1vN29p7t3dvfOLVu2rGZEAAAAFtCmle5YVVcl+btJXjGd2pvuPpzk8HT/zqp6IMnzs3QkdfZU4W1JDq70tQEAANjYVnRktaouT/KOJD/X3d+dWd9SVWdM938qSxdS+np3P5LkO1X14ukqwK9P8olVTw8AAMCGdMIjq1V1U5JLk2yuqgNJrsnS1X/PTHLH9C/QfGG68u/Lk7y7qh5P8kSSN3f3kYszvSVLVxY+K0vvcZ19nysAAAD8wAljtbuvXGb5Q8fY9tYktx7jsb1JLnpa0wEAAHBamsfVgAEAAGCuxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMMRqwAAAAxHrAIAADAcsQoAAMBwxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMMRqwAAAAxHrAIAADAcsQoAAMBwxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMMRqwAAAAxHrAIAADAcsQoAAMBwxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMMRqwAAAAxHrAIAADAcsQoAAMBwThirVXVDVT1WVffMrD27qu6oqq9NH58189jVVbW/qu6vqstm1i+uqrunx95fVTX/LwcAAICN4GSOrH44yeVHrb0zyWe6+4Ikn5k+T1VdmGRXkhdO+3ygqs6Y9rk+ye4kF0y3o58TAAAAkpxErHb355J866jlK5J8ZLr/kSSvnlm/ubsPd/eDSfYnuaSqzklydnd/vrs7yY0z+wAAAMCTrPQ9q8/t7keSZPr4nGl9a5KHZ7Y7MK1tne4fvQ4AAABPMe8LLC33PtQ+zvryT1K1u6r2VtXeQ4cOzW04AAAAFsNKY/XR6dTeTB8fm9YPJDl3ZrttSQ5O69uWWV9Wd+/p7p3dvXPLli0rHBEAAIBFtdJYvS3JVdP9q5J8YmZ9V1WdWVXnZ+lCSl+cThX+TlW9eLoK8Otn9gEAAIAn2XSiDarqpiSXJtlcVQeSXJPkXya5paremOTPkrw2Sbr73qq6Jcl9SR5P8tbufmJ6qrdk6crCZyX55HQDAACApzhhrHb3lcd46BXH2P66JNcts743yUVPazoAAABOS/O+wBIAAACsmlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABiOWAUAAGA4YhUAAIDhiFUAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABiOWAUAAGA4YhUAAIDhiFUAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABiOWAUAAGA4YhUAAIDhiFUAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABiOWAUAAGA4YhUAAIDhiFUAAACGI1YBAAAYzopjtapeUFX7Zm5/UVVvr6prq+obM+uvmtnn6qraX1X3V9Vl8/kSAAAA2Gg2rXTH7r4/yY4kqaozknwjyceT/HyS93X3e2a3r6oLk+xK8sIkP5nk96vq+d39xEpnAAAAYGOa12nAr0jyQHf/9+Nsc0WSm7v7cHc/mGR/kkvm9PoAAABsIPOK1V1Jbpr5/G1VdVdV3VBVz5rWtiZ5eGabA9MaAAAAPMmqY7WqfjjJzyX5rWnp+iTPy9Ipwo8kee+RTZfZvY/xnLuram9V7T106NBqRwQAAGDBzOPI6iuTfLm7H02S7n60u5/o7u8n+WD+8lTfA0nOndlvW5KDyz1hd+/p7p3dvXPLli1zGBEAAIBFMo9YvTIzpwBX1Tkzj70myT3T/duS7KqqM6vq/CQXJPniHF4fAACADWbFVwNOkqr60SQ/m+RNM8v/qqp2ZOkU34eOPNbd91bVLUnuS/J4kre6EjAAAADLWVWsdvd3k/zVo9Zed5ztr0ty3WpeEwAAgI1vXlcDBgAAgLkRqwAAAAxHrAIAADAcsQoAAMBwxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMMRqwAAAAxHrAIAADAcsQoAAMBwxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMMRqwAAAAxHrAIAADAcsQoAAMBwxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMMRqwAAAAxHrAIAADAcsQoAAMBwxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMNZVaxW1UNVdXdV7auqvdPas6vqjqr62vTxWTPbX11V+6vq/qq6bLXDAwAAsDHN48jqT3f3ju7eOX3+ziSf6e4Lknxm+jxVdWGSXUlemOTyJB+oqjPm8PoAAABsMKfiNOArknxkuv+RJK+eWb+5uw9394NJ9ie55BS8PgAAAAtutbHaSX6vqu6sqt3T2nO7+5EkmT4+Z1rfmuThmX0PTGsAAADwJJtWuf9Lu/tgVT0nyR1V9dXjbFvLrPWyGy6F7+4kOe+881Y5IgAAAItmVUdWu/vg9PGxJB/P0mm9j1bVOUkyfXxs2vxAknNndt+W5OAxnndPd+/s7p1btmxZzYgAAAAsoBXHalX9lar68SP3k/ydJPckuS3JVdNmVyX5xHT/tiS7qurMqjo/yQVJvrjS1wcAAGDjWs1pwM9N8vGqOvI8H+vuT1XVl5LcUlVvTPJnSV6bJN19b1XdkuS+JI8neWt3P7Gq6QEAANiQVhyr3f31JH9zmfX/keQVx9jnuiTXrfQ1AQAAOD2cin+6BgAAAFZFrAIAADAcsQoAAMBwxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMMRqwAAAAxHrAIAADAcsQoAAMBwxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMMRqwAAAAxHrAIAADAcsQoAAMBwxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMMRqwAAAAxHrAIAADAcsQoAAMBwxCoAAADDEasAAAAMR6wCAAAwHLEKAADAcMQqAAAAwxGrAAAADEesAgAAMByxCgAAwHDEKgAAAMMRqwAAAAxnxbFaVedW1X+pqq9U1b1V9c+m9Wur6htVtW+6vWpmn6uran9V3V9Vl83jCwAAAGDj2bSKfR9P8svd/eWq+vEkd1bVHdNj7+vu98xuXFUXJtmV5IVJfjLJ71fV87v7iVXMAAAAwAa04iOr3f1Id395uv+dJF9JsvU4u1yR5ObuPtzdDybZn+SSlb4+AAAAG9dc3rNaVduTvCjJH01Lb6uqu6rqhqp61rS2NcnDM7sdyPHjFgAAgNPUqmO1qn4sya1J3t7df5Hk+iTPS7IjySNJ3ntk02V272M85+6q2ltVew8dOrTaEQEAAFgwq3nPaqrqh7IUqh/t7t9Oku5+dObxDyb53enTA0nOndl9W5KDyz1vd+9JsidJdu7cuWzQsrHUu5b7bxlrq6/xowYAAKNYzdWAK8mHknylu//1zPo5M5u9Jsk90/3bkuyqqjOr6vwkFyT54kpfHwAAgI1rNUdWX5rkdUnurqp909qvJLmyqnZk6RTfh5K8KUm6+96quiXJfVm6kvBbXQmYReLoLwAArJ0Vx2p3/0GWfx/q7cfZ57ok1630NQEAADg9zOVqwAAAADBPq7rAEjAWpyoDALBROLIKAADAcMQqAAAAw3EaMLCmnKoMAMDJcGQVAACA4YhVAAAAhiNWAQAAGI73rAIcxftqAQDWn1gFWECCGgDY6JwGDAAAwHAcWQXglHD0FwBYDUdWAQAAGI5YBQAAYDhiFQAAgOF4zyoAp61FeF/tIswIAKeCI6sAAAAMx5FVAGBVHP0F4FQQqwDAhieoARaPWAUAGMAiBPUizAhsHGIVAIANQ1DDxiFWAQBgDQlqODliFQAAeBJBzQjEKgAAsHAE9cYnVgEAAE4BQb06z1jvAQAAAOBoYhUAAIDhiFUAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABiOWAUAAGA4YhUAAIDhiFUAAACGI1YBAAAYjlgFAABgOGIVAACA4YhVAAAAhiNWAQAAGI5YBQAAYDhiFQAAgOGIVQAAAIYjVgEAABjOmsdqVV1eVfdX1f6qeudavz4AAADjW9NYraozkvy7JK9McmGSK6vqwrWcAQAAgPGt9ZHVS5Ls7+6vd/f/TXJzkivWeAYAAAAGt9axujXJwzOfH5jWAAAA4Aequ9fuxapem+Sy7v6F6fPXJbmku3/xqO12J9k9ffqCJPev2ZDrY3OSb673ECdgxvkw43yYcT7MOB9mnA8zzocZ58OM82HG+ViEGVfjm0nS3Zcf/cCmNR7kQJJzZz7fluTg0Rt1954ke9ZqqPVWVXu7e+d6z3E8ZpwPM86HGefDjPNhxvkw43yYcT7MOB9mnI9FmPFUWevTgL+U5IKqOr+qfjjJriS3rfEMAAAADG5Nj6x29+NV9bYkn05yRpIbuvvetZwBAACA8a31acDp7tuT3L7Wrzu4RTjl2YzzYcb5MON8mHE+zDgfZpwPM86HGefDjPOxCDOeEmt6gSUAAAA4GWv9nlUAAAA4IbHKhlNVt1fVM9d7Dk4fVfUrM/e3V9U96zkPAMBG4DRggFWqqv/V3T823d+e5He7+6J1HQoAYMGt+QWWTmdVdW2SFyd5fFralOQLy61197VrPV+yGDPOqqr/nKV/u/dHkvyb7t5TVQ8l2dnda/qPJz+d7910f5jv6bFmX+8/4/X6eZyC81NJ/ijJi5L8aZLfSPIL3f2aaZufTfKW6bGzqmpfknuT/GqSM6rqg0n+VpJvJLmiu79XVTuS/HqSH03yQJI3dPe3q+q/Tq/100memeSN3f3f5vj1XJvBf68XYcZZo/7OzBp1xkX4szbjfCzCjLP8zszHqfw+zuPvWqd6fV5/BqP+PK41sbr2dnX3/0yS6VTVtx9jbT0twoxHvKG7v1VVZyX5UlXdus7zPJ3v3Wjf09HmOWK9fh5fkKVo/MOquiHJhUn+elVt6e5DSX4+yW909+9U1du6e8c0z/YkFyS5srv/SVXdkuTvJ/nNJDcm+cXu/mxVvTvJNTOzb+ruS6rqVdP6z8z561mE3+tFmHHWyLMdMeqMi/Bnbcb5WIQZZ406m+/j8Z/76f5d61Svn8qv9bTiPassun9aVX+Spf/CdW6WIgHm4eHu/sPp/m8meWmS/5DkH03/h/GSJJ88xr4Pdve+6f6dSbZX1U8keWZ3f3Za/0iSl8/s89uz28/nSwAAWFyOrLKwqurSLB19ekl3f3c6lfJH1nUoNpKj39DfWToV+HeS/J8kv9Xdjz9lryWHZ+4/keSsk3i9I/s8Ef/bDADgyCoL7SeSfHsK1b+WpfP6YV7Oq6qXTPevTPIH3X0wycEk/zzJh2e2/X9V9UPHe7Lu/vMk366ql01Lr0vy2ePsAgBwWhOrLLJPJdlUVXcl+Rf5yze7wzx8JclV08/Xs5NcP61/NEunCN83s+2eJHdV1UdP8JxXJfm16Tl3JHn3nGcGANgwnGrGwuruw0leucxD29d4FDam73f3m5dZ/9tJPji70N3vSPKOmaWLZh57z8z9fVnmDIDuvnTm/jfjZxgAQKwCnKyqujPJ/07yy+s9CwDARidW19ZjSW6squ9Pnz8jS6eyLre2XhZhxlE93e/dSN/TY82+3tbl57G7H8rM0dGZ9Yvn+TpraBF+rxdhxlmj/s7MGnXGRfizNuN8LMKMs/zOzMep/D7O6+9ap3p9Hkb9eVxT1X30BS8BAABgfbnAEgAAAMMRqwAAAAxHrAIAADAcsQoAAMBwxCoAAADD+f8Wr8j4YV07YgAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"As I sort of expected, Matplotlib does not recognize Japanese characters. It is the whole reason I wanted to test this."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"The spacy Japanese is pretty strong. Of course, I tried NLTK Japanese and could not get it to work at all, so at the very least, getting spacy up and working is pretty big. In terms of processing, Japanese is considered a big, single block by functions like split(). This thus requires tokenizing followed by getting the individual word out in order to actually process the text. And even when tokenized, words like 機械学習 (Machine Learning) that are made of two words are split into the two words, which is something to look out for. English may have concepts like this that are two words like machine learning, but Japanese words like this this often hold more meaning together than as the sum of their parts. \n\nThis is followed by a check with the stopwords, which does not include some words that I would consider stopwords, so that is another thing to look out for when processing Japanese. One more key part of this is matplotlib, as it does not recognize Japanese characters at all. It just leaves boxes in the place of a character. This means numeric data like model accuracy hold a lot more value when working with Japanese than categorical data that would typically be powerful during exploration. \n\nAs for the dataset itself, it is pretty barebones. It is machine learning, so the most common words (besides those I would consider stopwords), are expectedly machine, learning, AI, programming, and python. There are also words like ability (可能) and world (世界), so likely some optimistic discussion on changing the world and learning ability, similar to how it is discussed in English. "}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}