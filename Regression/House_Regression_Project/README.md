# House_Regression_Project--Kaggle

![Accuracies](https://github.com/Luna-McBride/Kaggle_Personal_Projects/blob/master/Regression/House_Regression_Project/Accuracies.png)

Data Used: https://www.kaggle.com/swathiachath/kc-housesales-data

The Notebook in Kaggle: https://www.kaggle.com/lunamcbride24/house-regression-project

My Kaggle: https://www.kaggle.com/lunamcbride24

This project experimented with various regression algorithms from Scikit-Learn. The data itself was not all that messy, so the only cleanup that needed to be done was removing some unnecessary columns and using dates to make an age column.

When it came to the different models' accuracies, the forests tended to stay ahead, with the SMV/Neural Network close behind, then a big gap between them and the Linear Regression and the single decision tree. On my initial run through, the forests were at around 88% with the Neural Network at 86% and the SMV at about 84%, but the rerun shown above made the forests, neural network, and SMV very close together. That emphasizes that any of those are strong regression tools. This also puts the Linear Regression pretty far behind its peers, but since this is something I was able to code by hand before in my Data Science class, I sort of expected a little desparity between it and some of these other raw power algorithms

After training the models, I looked through the coefficients weights generated by the Linear regression and compared them to the strongest of the initial run, the random forest, for what they considered the most important factors in determining the price. Both of the outputs are pictured below.

![LinearFactors](https://github.com/Luna-McBride/Kaggle_Personal_Projects/blob/master/Regression/House_Regression_Project/LinearFactors.png)

![ForestFactors](https://github.com/Luna-McBride/Kaggle_Personal_Projects/blob/master/Regression/House_Regression_Project/ForestFactors.png)

What is interesting about these is the desparity between the choices the forest and Linear Regression picked, as well as what they both had in common.

* The two regressions both picked grade as a key factor. In this dataset, grade is the evaluation given by King County (where this data is from) on how good the house is. It is such a concrete variable to be linked with price that I would have been surprised if it was not in the top five.

* The Linear Regression took stronger liking to the two variables that indicated age. Which I could see that being an excellent choice if it is either an older or newer house, but not the top two for the whole dataset. I looked through to see where the forest put age and it was at slot 9, so a stark contrast between the two models.

* Both regressions included latitude, but the forest also included longitude. This showed that both picked up on the importance of location when it came to choosing the factors, which is to be expected when thinking about house prices versus wealthier areas. Yet, since the Linear one took so much emphasis on age, it only got one of the location components while the forest got both.

* Both regressions also included house square footage measures in the top five, but once again, the forest got both the overall square footage and the square footage of the house above ground (not counting the basement) while the Linear Regression only got the above ground square footage. This means both picked up that the size matters in the calculations, but again with the Linear Regression taking more focus on the age of the house rather than the size.
